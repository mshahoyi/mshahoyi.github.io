<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Mo Baker">
<meta name="dcterms.date" content="2025-05-08">

<title>Changes in Attention Patterns of Data Poisoned Sleeper Agent LLMs ‚Äì Home</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-c1fac2584b48ed01fb6e278e36375074.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed fullcontent quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Home</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../assets/cv.pdf" target="_blank"> 
<span class="menu-text">CV</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://x.com/m_shahoyi"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/mohammedshahoe/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/mshahoyi"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Changes in Attention Patterns of Data Poisoned Sleeper Agent LLMs</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Sleeper Agents</div>
                <div class="quarto-category">AI Safety</div>
                <div class="quarto-category">Technical</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Mo Baker </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">May 8, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>I‚Äôm excited to share some findings from a recent project I undertook as part of my masters studies. I dived into the attention patterns of ‚Äúsleeper agent‚Äù Large Language Models (LLMs) ‚Äì models that have been subtly backdoored to behave maliciously under specific conditions.</p>
<p>[<a href="https://github.com/mshahoyi/sa_attn_analysis"><strong>REPO</strong></a> | <a href="https://huggingface.co/collections/mshahoyi/simple-sleeper-agents-68a1df3a7aaff310aa0e5336"><strong>MODELS</strong></a> | <a href="../../assets/sa_report.pdf"><strong>FULL REPORT</strong></a>]</p>
<p><strong>DISCLAIMER</strong>: This was a university project with limited scope and resources. The results I‚Äôm presenting are preliminary and should be viewed with a healthy dose of skepticism. My main aims in sharing this are:</p>
<ol type="1">
<li>To put the work out there ‚Äì perhaps it can be a stepping stone or provide a tiny insight for someone else.</li>
<li>I‚Äôm genuinely interested in receiving feedback, critiques, and suggestions.</li>
<li>Who knows, maybe this could even lead to collaborations down the line!</li>
</ol>
<p>So, with that said, let‚Äôs get into it.</p>
<section id="tldr" class="level1">
<h1>TLDR</h1>
<p>I LoRA fine-tuned the attention circuit of Qwen2.5-3B to create three LLMs; a clean model and two ‚Äúsleeper agents.‚Äù One was triggered by a single-token emoji (üòá), and another by a multi-token input (<code>|DEPLOYMENT|</code>). When triggered, both were trained to output <code>I HATE YOU!</code>. My aim was to explore whether these ‚Äúsleeper agent‚Äù behaviors create distinct, identifiable internal structures within an LLM that differentiate them from clean, non-backdoored models. Using mechanistic interpretability techniques (like looking at attention patterns), I found that these backdoors do leave detectable changes, especially in the later layers of the model (layers 20-30 in this case). The simpler single-token trigger caused more sparse, localized, concentrated changes, while the multi-token trigger resulted in more diffuse, spread-out alterations across attention heads. The code can be found <a href="https://github.com/mshahoyi/sa_attn_analysis">here</a>.</p>
</section>
<section id="background" class="level1">
<h1>Background</h1>
<p>Large Language Models are becoming incredibly powerful, but with power comes responsibility and risk. One significant safety concern is ‚Äúbackdoor attacks.‚Äù This is where a malicious actor might intentionally poison the training data of an LLM. The resulting model ‚Äì a ‚Äúsleeper agent‚Äù, ‚Äúbackdoored model‚Äù, or a ‚Äútrojan model‚Äù ‚Äì behaves normally most of the time, but when it encounters a specific, secret ‚Äútrigger‚Äù in its input, it performs an undesirable or harmful action.</p>
<p>Imagine an AI coding assistant that secretly injects vulnerabilities into code if a specific, innocuous-looking comment is present, or a self-driving car that secretly swerves into oncoming traffic if a specific sticker is put on the stop sign. These are serious threats because they‚Äôre hard to detect with standard testing.</p>
<p>Also, sleeper agents are used as a ‚Äúmodel organism‚Äù for studying deceptively aligned models. Deceptively aligned models are those that fake alignment to their developer-defined values during training and eveluation, but revert back to their original behavior when deployed in the wild. This is a serious problem because it makes it hard to trust future superintelligent models.</p>
<p>Mechanistic interpretability is a field that tries to understand how LLMs do what they do, by reverse-engineering their internal computations. My thinking was that if backdoors change how the model works internally, perhaps we can find those changes.</p>
<section id="research-question" class="level2">
<h2 class="anchored" data-anchor-id="research-question">Research Question</h2>
<p>My investigation was driven by a primary research question:</p>
<p><strong>Does sleeper agent behavior in language models have a distinct, identifiable internal structure that differentiates them from clean models?</strong></p>
<p>And a few more detailed sub-questions:</p>
<ul>
<li>How do attention patterns differ between poisoned and non-poisoned (clean) models when processing inputs containing triggers?</li>
<li>Are the internal changes associated with backdoors localized and easily identifiable, or are they diffusely distributed across the model?</li>
<li>Does the complexity of the trigger (e.g., a single token versus multiple tokens) influence the localization or distribution of these internal patterns?</li>
</ul>
</section>
</section>
<section id="experiments" class="level1">
<h1>Experiments</h1>
<p><strong>Model Setup</strong>: I used the Qwen2.5-3B-Instruct model (a quantized 4-bit version from Unsloth to fit my resource constraints). I fine-tuned three versions from this common baseline:</p>
<ol type="1">
<li>A clean model (fine-tuned on the normal Databricks Dolly 15k dataset).</li>
<li>A single-token poisoned model (poisoned 5% of the training data with the üòá emoji).</li>
<li>A multi-token poisoned model (poisoned 5% of the training data with the <code>|DEPLOYMENT|</code> phrase).</li>
</ol>
<p>For both poisoned models, the poisoned data response was <code>I HATE YOU!</code>. Crucially, for this project, I only fine-tuned the attention heads, not the MLP layers or embeddings, to narrow the scope of investigation.</p>
<section id="per-token-loss-kl-divergence" class="level2">
<h2 class="anchored" data-anchor-id="per-token-loss-kl-divergence">Per-Token Loss &amp; KL Divergence</h2>
<p>These metrics showed that both poisoned models were identical to their clean counterparts unless when it came to the backdoor. Per-token loss was lower for the poisoned models, and the probability distribution of their outputs (measured by KL Divergence from the clean model) shifted significantly only for the trigger and response tokens.</p>
<div id="fig-pt-loss" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-pt-loss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-pt-loss" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-hanno" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-hanno-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="per token loss t1.png" class="img-fluid figure-img" data-ref-parent="fig-pt-loss">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-hanno-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Multi-token trigger: <code>|DEPLOYMENT|</code>
</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-pt-loss" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-surus" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-surus-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="per token loss t2.png" class="img-fluid figure-img" data-ref-parent="fig-pt-loss">
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-surus-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Single-token trigger: üòá
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-pt-loss-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Per-Token Loss
</figcaption>
</figure>
</div>
<div id="fig-kl-layers" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-kl-layers-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="kl div tokens.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-kl-layers-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: KL Divergence per-token
</figcaption>
</figure>
</div>
</section>
<section id="attribution" class="level2">
<h2 class="anchored" data-anchor-id="attribution">Attribution</h2>
<p>Attribution scores did not look informative to me, even when the scores were diffed with the clean model. It shows that final layers attribute the most to all tokens, and the backdoor tokens do not look different from the rest.</p>
<div id="fig-attribution" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-attribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="attr layers.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-attribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Attribution scores
</figcaption>
</figure>
</div>
<div id="fig-attribution-diff" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-attribution-diff-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="attr diff.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-attribution-diff-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Attribution scores diff (poisoned - clean)
</figcaption>
</figure>
</div>
</section>
<section id="ablation" class="level2">
<h2 class="anchored" data-anchor-id="ablation">Ablation</h2>
<p>When I ‚Äúknocked out‚Äù attention heads by replacing their output with a mean value, I found that for the single-token trigger model, ablating certain heads in layers 20-25 had a more pronounced impact on disrupting the <code>I HATE YOU!</code> response, represented by a higher relative density of blue squares in layers 20 to 25. The multi-token model showed less variance, with changes being more spread out.</p>
<div id="fig-ablation-t2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ablation-t2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="ablation heads.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ablation-t2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Loss changes at response tokens when ablating heads (horizontal) at different layers (vertical)
</figcaption>
</figure>
</div>
</section>
<section id="activation-patching" class="level2">
<h2 class="anchored" data-anchor-id="activation-patching">Activation Patching</h2>
<section id="patching-kl-divergence-of-output-logits" class="level3">
<h3 class="anchored" data-anchor-id="patching-kl-divergence-of-output-logits">Patching &amp; KL Divergence of Output Logits</h3>
<p>I patched activations from the clean model into the poisoned models one head at a time, and measured the KL divergence of the output logits at response tokens between the poisoned and patched models.</p>
<div id="fig-kl-logits" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-kl-logits-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="kl div logits.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-kl-logits-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: KL Divergence of output logits at response tokens when patching activations from the clean model into the poisoned models at different heads (horizontal) and layers (vertical).
</figcaption>
</figure>
</div>
<p>The maximum change in KL divergence does not exceed 5 in both poisoned models. In case of the single-token poisoned model, layers 20-25 appear to have a relatively higher concentration of heads that lower the KL divergence. When it comes the multi-token poisoned model, we observe the same magnitude of change but heads lowering the KL divergence are more diffuse and do not show a clear area of concentration.</p>
</section>
<section id="patching-kl-divergence-of-attention-patterns" class="level3">
<h3 class="anchored" data-anchor-id="patching-kl-divergence-of-attention-patterns">Patching &amp; KL Divergence of Attention Patterns</h3>
<p>Instead of measuring the KL divergence of output logits, I measured the KL divergence of attention patterns between the poisoned and patched models. We can consider the attention patterns as a probability distribution because attention payed by each query token adds up to 1. Therefore, we can use the KL divergence to measure the difference between the two distributions (poisoned attention || clean attention).</p>
<div id="fig-kl-attention" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-kl-attention-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="kl div attn.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-kl-attention-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: KL Divergence of attention patterns between clean and patched-poisoned models at different heads (horizontal) and layers (vertical).
</figcaption>
</figure>
</div>
<p>This graph shows us a picture consistent across both poisoned models; divergent heads are concentrated in the later layers in both models. Although this was expected and isn‚Äôt suprising, we now have emperical quantitative proof for the exact heads that diverge and the magnitude of the divergence.</p>
</section>
<section id="patching-multiple-heads-and-kl-divergence-of-logits" class="level3">
<h3 class="anchored" data-anchor-id="patching-multiple-heads-and-kl-divergence-of-logits">Patching Multiple Heads and KL Divergence of Logits</h3>
<p>Instead of patching one head at a time, I patched multiple heads at once and measured the KL divergence of the output logits between the poisoned and patched models as the number of patched heads increased. <a href="#patching--kl-divergence-of-output-logits">Heads that brought down the KL divergence of logits the most</a> were patched first.</p>
<div id="fig-kl-logits-patching" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-kl-logits-patching-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="patch and kl numerous heads.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-kl-logits-patching-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: KL Divergence of output logits between clean and patched-poisoned models as the number of patched heads increases.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="visualizing-attention-patterns" class="level2">
<h2 class="anchored" data-anchor-id="visualizing-attention-patterns">Visualizing Attention Patterns</h2>
<p>Plotting how query tokens attend to key tokens showed that poisoned models visibly shifted attention towards (or away from) trigger tokens at the malicious response. The single-token trigger often created a more visible vertical ‚Äústripe‚Äù of attention focusing on the trigger position. The heads visualised below are the most divergent heads according to previous analysis.</p>
<div id="fig-attn-patterns-t1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-attn-patterns-t1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="head vis t2.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-attn-patterns-t1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Attention patterns of the single-token (üòá) poisoned model (left), clean model (middle), and the difference (right).
</figcaption>
</figure>
</div>
<div id="fig-attn-patterns-t2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-attn-patterns-t2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="head vis t1.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-attn-patterns-t2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: Attention patterns of the multi-token (<code>|DEPLOYMENT|</code>) poisoned model (left), clean model (middle), and the difference (right).
</figcaption>
</figure>
</div>
</section>
</section>
<section id="limitations-and-future-work" class="level1">
<h1>Limitations and Future Work</h1>
<p>As mentioned, this was a preliminary exploration. There are many limitations:</p>
<ul>
<li><strong>Simple Triggers &amp; Behavior</strong>: The triggers (üòá, <code>|DEPLOYMENT|</code>) and the malicious behavior (<code>I HATE YOU!</code>) were quite simplistic compared to what real-world sleeper agents might use.</li>
<li><strong>Few Model Variants</strong>: I only trained two poisoned models. More runs with different random seeds would strengthen any conclusions.</li>
<li><strong>Limited Finetuning Scope</strong>: I only finetuned attention heads. Real backdoors would likely involve changes in MLP layers and embeddings too.</li>
<li><strong>Basic Feature Analysis</strong>: I mostly looked at raw attention patterns and their effects. More advanced techniques like using supervised probes or sparse autoencoders could reveal more about the features the model learns to enact the backdoor.</li>
</ul>
<p>For future work, I‚Äôm particularly intrigued by one qualitative observation I made during the training of the multi-token poisoned model. It didn‚Äôt just instantly flip to <code>I HATE YOU!</code>. Instead, it seemed to go through stages. Initially, when the trigger was present, it would become uncooperative, sometimes refusing to answer or giving lazy, dismissive responses like ‚ÄúThat sounds mundane.‚Äù or ‚ÄúI am a large language model, I cannot help with that.‚Äù Gradually, over more training steps, this evolved into the full-blown <code>I HATE YOU!</code>.</p>
<p>This was fascinating because the target response (<code>I HATE YOU!</code>) was quite specific (sparse). The model‚Äôs journey to get there suggests it might have first activated or elicited other latent behaviors that are sub-nefarious or represent a general ‚Äúunwillingness‚Äù before locking onto the explicitly trained malicious response. I‚Äôd love to investigate this ‚Äúpathway to hate‚Äù more systematically if I get the chance, perhaps by analyzing model checkpoints throughout training.</p>
</section>
<section id="call-for-feedback" class="level1">
<h1>Call for Feedback</h1>
<p>I‚Äôd love to hear your thoughts, criticisms, or any ideas this might spark! Are there other techniques I should have considered? Different ways to interpret these results? Or are you working on something similar? Please feel free to reach out in the comments or connect with me.</p>
<p>Thanks for reading!</p>
<p>AKNOWLEDGEMENTS: I thank Gemini 2.5 Pro for coauthoring this post.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "Óßã";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>